# Анализ и Отчетность по Ревью Кода с Помощью ИИ

Этот репозиторий содержит систему для анализа качества кода в Merge Requests (MR) с использованием ИИ (Больших Языковых Моделей - LLM) и генерации структурированных отчетов. Система решает проблемы, возникающие при использовании LLM исключительно для количественной оценки, разделяя качественный анализ (выполняемый LLM) и количественные расчеты (выполняемые отдельными скриптами).

## Проблематика

Первоначальное тестирование выявило несколько проблем при прямом использовании LLM для сквозной генерации отчетов по ревью кода:

1.  **Плохой расчет средних оценок:** LLM испытывали трудности с точным расчетом средних оценок при обработке разнообразных данных метрик из нескольких MR.
2.  **Нестабильные агрегированные оценки:** Итоговая оценка по метрикам в сводном отчете (`employee_report`) различалась от запуска к запуску, что ненадежно.
3.  **Непоследовательная генерация отчетов:** Сгенерированные Markdown-отчеты не всегда были идентичны для одних и тех же входных данных.
4.  **Отсутствие учета общего качества кода:** Система неэффективно учитывала общее низкое качество кода, выходящее за рамки конкретных метрик.
5.  **Снижение точности при увеличении сложности:** Генерация множества различных типов данных (оценок, резюме, анализов для нескольких метрик) в одном запросе к LLM снижало общую точность.

## Реализованное Решение

Для решения этих проблем рабочий процесс был переработан:

1.  **Разделение Ответственности:**
    *   LLM используются для качественного анализа: выявления паттернов, генерации резюме, детального анализа и рекомендаций на основе предоставленных определений метрик и diff кода.
    *   Расчет оценок (взвешенных средних) делегирован отдельным скриптам (`calc_weights.py`, `calcWeights.ts`).
2.  **Двухэтапная Агрегация Отчетов:** Генерация `employee_report` разделена на два этапа:
    *   `metric_summary`: LLM агрегирует текстовые описания (резюме, анализ, рекомендации) для каждой метрики по нескольким MR. **Важно: на этом шаге оценки не рассчитываются.**
    *   `total_summary`: Отдельный промпт LLM генерирует общую качественную оценку (`overall_assessment`, `positives`, `areas_for_improvement`) на основе агрегированного текста и рассчитанных оценок.
3.  **Внешний Расчет Оценок:** Скрипты рассчитывают взвешенные средние оценки (`aggregatedScore`) для каждой метрики (`antiPatterns`, `codeStyle`, `designPatterns`) на основе:
    *   Исходной оценки, данной LLM для этой метрики в конкретном MR.
    *   Классификации сложности MR (`Low`, `Medium`, `High`).
    *   Уровня уверенности LLM (`Low`, `Medium`, `High`) в своей оценке.
4.  **Настраиваемые Веса:** Влияние сложности MR и уверенности LLM на итоговую оценку можно настроить с помощью весов, что позволяет адаптировать систему под конкретный проект:
    *   `complexity_weight = {'Low': 1.0, 'Medium': 1.5, 'High': 2.0}` (MR с более высокой сложностью имеют больший вес оценки).
    *   `confidence_weight = {'Low': 0.75, 'Medium': 1.0, 'High': 1.25}` (Оценки с более высоким уровнем уверенности вносят больший вклад во взвешенную оценку).
    *   Это позволяет сильнее штрафовать за "плохой код" в сложных MR или придавать больший вес оценкам с высокой уверенностью.
5.  **Генерация Отчетов по Шаблону:** Итоговый Markdown-отчет генерируется путем заполнения шаблона (`employee_report_template.md`) агрегированным текстом от LLM и рассчитанными оценками из скриптов.

## Итоговый Рабочий Процесс

Процесс генерации сводного отчета для нескольких MR выглядит следующим образом:

**I. Анализ для Каждого MR (Повторяется для каждого MR):**

1.  **Запуск Оценки Метрик:** Используйте LLM с промптами из соответствующих директорий для анализа diff MR:
    *   `complexity`: Классифицировать сложность (`Low`/`Medium`/`High`) и предоставить обоснование.
    *   `anti_patterns`: Оценка (1-10), анализ и рекомендации.
    *   `code_style`: Оценка (1-10), анализ и рекомендации.
    *   `design_patterns`: Оценка (1-10), анализ и рекомендации.
    *   *(Сохраните JSON-вывод для каждой оценки метрики)*.
2.  **Запуск Отчета по MR:** (Опционально) Используйте промпты `mr_report` для генерации краткого, удобочитаемого резюме отдельного MR, интегрируя результаты метрик.

**II. Генерация Сводного Отчета (Используя данные всех проанализированных MR):**

3.  **Запуск Агрегации Резюме по Метрикам:**
    *   Скомпилируйте JSON-выводы из Шага 1 для всех MR в структуру, подобную `pull_reviews.json`.
    *   Используйте промпт `employee_report/assistant_metrics_summary.md` с скомпилированным JSON в качестве входных данных.
    *   **Результат:** JSON-объект, содержащий агрегированные *текстовые* резюме, анализы и рекомендации для каждой метрики (например, `example_metrics_summary.json`, но **изначально *без* полей `score`**).
4.  **Расчет Взвешенных Оценок:**
    *   Запустите `calc_weights.py` или `calcWeights.ts`, используя скомпилированные JSON-данные (`pull_reviews.json`) в качестве входных данных.
    *   **Результат:** JSON-объект, содержащий рассчитанные `aggregatedScore` (взвешенное среднее) для каждой метрики (`antiPatterns`, `codeStyle`, `designPatterns`), а также детали расчета.
    *   *Объедините (вставьте) эти рассчитанные значения `aggregatedScore` в JSON, сгенерированный на Шаге 3.*
5.  **Запуск Генерации Итогового Резюме:**
    *   Используйте промпт `employee_report/assistant_total_summary.md`.
    *   Предоставьте *объединенный* JSON из Шагов 3 и 4 (содержащий как агрегированный текст, так и рассчитанные оценки) в качестве входных данных.
    *   **Результат:** JSON-объект, содержащий общую качественную оценку (`overall_assessment`, `positives`, `areas_for_improvement`) (например, `example_total_summary.json`).
6.  **Расчет Финальной Агрегированной Оценки:**
    *   Рассчитайте общую `aggregatedTotalScore` на основе значений `aggregatedScore` отдельных метрик, рассчитанных на Шаге 4. Это может быть простое среднее или другое взвешенное среднее в зависимости от желаемого акцента (этот расчет не реализован в предоставленных скриптах, но ожидается шаблоном).
7.  **Генерация Markdown Отчета:**
    *   Используйте шаблон `employee_report/employee_report_template.md`.
    *   Подставьте плейсхолдеры соответствующими данными, собранными на Шагах 3, 4 (объединенные), 5 и 6.

## Структура Проекта

```
├── anti_patterns/              # Промпты LLM и пример для метрики Anti-Patterns
│   ├── assistant.md
│   ├── example.json
│   └── system.md
├── code_style/                 # Промпты LLM и пример для метрики Code Style
│   ├── assistant.md
│   ├── example.json
│   └── system.md
├── complexity/                 # Промпты LLM и пример для классификации Complexity
│   ├── assistant.md
│   ├── example.json
│   └── system.md
├── design_patterns/            # Промпты LLM и пример для метрики Design Patterns
│   ├── assistant.md
│   ├── example.json
│   └── system.md
├── employee_report/            # Компоненты для генерации итогового сводного отчета
│   ├── assistant_metrics_summary.md # Промпт LLM для агрегации текстовых резюме по метрикам
│   ├── assistant_total_summary.md   # Промпт LLM для общей качественной оценки
│   ├── employee_report_template.md  # Markdown-шаблон для итогового отчета
│   ├── example_metrics_summary.json # Пример вывода агрегации текста метрик + рассчитанные оценки
│   ├── example_total_summary.json   # Пример вывода общей качественной оценки
│   ├── system_metrics_summary.md    # Системный промпт для агрегации резюме метрик
│   └── system_total_summary.md      # Системный промпт для генерации итогового резюме
├── mr_report/                  # Промпты LLM для генерации резюме по отдельным MR
│   ├── assistant.md
│   └── system.md
├── calc_weights.py             # Python-скрипт для расчета взвешенных средних оценок по метрикам
├── calcWeights.ts              # TypeScript-скрипт (эквивалент Python-скрипта)
├── pull_reviews.json           # Пример входных данных: массив ревью отдельных MR
└── README.md                   # Этот файл (переведенная версия)
```

## Ключевые Компоненты

*   **Директории Метрик (`anti_patterns`, `code_style`, `complexity`, `design_patterns`):** Содержат промпты LLM (`assistant.md`, `system.md`), определяющие метрику, критерии оценки и желаемый формат вывода JSON, а также пример вывода (`example.json`). Используются для анализа отдельных MR.
*   **`mr_report/`:** Содержит промпты LLM для генерации удобочитаемого резюме для одного MR, включающего результаты оценок метрик.
*   **`employee_report/`:** Содержит промпты LLM для агрегации результатов по нескольким MR (`assistant_metrics_summary.md` для текста, `assistant_total_summary.md` для общей оценки), примеры вывода и шаблон итогового Markdown-отчета (`employee_report_template.md`).
*   **`calc_weights.py` / `calcWeights.ts`:** Скрипты, которые принимают собранные ревью (`pull_reviews.json`) и рассчитывают взвешенную среднюю оценку для каждой метрики (`antiPatterns`, `codeStyle`, `designPatterns`) на основе весов сложности и уверенности.
*   **`pull_reviews.json`:** Пример файла, показывающий ожидаемую структуру входных данных для шагов агрегации (массив объектов ревью отдельных MR).

**Примечание:** Большинство промптов LLM предназначены для генерации выходного текста (резюме, анализ, рекомендации) на **русском языке**.
